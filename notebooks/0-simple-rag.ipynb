{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Useful Links -----\n",
    "# https://github.com/AkariAsai/self-rag/tree/main?tab=readme-ov-file#updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /cis/home/adesilva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "import wikipediaapi\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_article(title, lang=\"en\"):\n",
    "    \"\"\"Fetch the Wikipedia article text.\"\"\"\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(user_agent='ashwin', language=lang)\n",
    "    page = wiki_wiki.page(title)\n",
    "    \n",
    "    if not page.exists():\n",
    "        print(f\"Page '{title}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    return page.text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove unwanted formatting, citations, and excessive whitespace.\"\"\"\n",
    "    text = re.sub(r\"\\[\\d+\\]\", \"\", text)  # Remove citation references like [1], [2]\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n",
    "    return text\n",
    "\n",
    "def split_into_passages(text, sentences_per_passage=5):\n",
    "    \"\"\"Split text into passages based on sentence count.\"\"\"\n",
    "    sentences = sent_tokenize(text)  # Tokenize into sentences\n",
    "    passages = [\n",
    "        \" \".join(sentences[i : i + sentences_per_passage])\n",
    "        for i in range(0, len(sentences), sentences_per_passage)\n",
    "    ]\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    def __init__(self, language_model=\"meta-llama/Llama-2-7b-chat-hf\", \n",
    "                 embedding_model=\"facebook/contriever\",\n",
    "                 device=None):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system\n",
    "        \"\"\"\n",
    "        # Set device (GPU if available, otherwise CPU)\n",
    "        if device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load Llama model and tokenizer\n",
    "        try:\n",
    "            print(f\"Loading Llama model from {language_model}...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(language_model)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                language_model,\n",
    "                torch_dtype=torch.float16,\n",
    "            ).to(self.device)\n",
    "            print(\"Llama model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Llama model: {str(e)}\")\n",
    "            print(\"You may need to login with 'huggingface-cli login' and request access to Llama models\")\n",
    "            self.tokenizer = None\n",
    "            self.model = None\n",
    "        \n",
    "        # Load embedding model\n",
    "        try:\n",
    "            print(f\"Loading embedding model {embedding_model}...\")\n",
    "            self.embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "            self.embedding_model = AutoModel.from_pretrained(embedding_model).to(self.device)\n",
    "            print(\"Embedding model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading embedding model: {str(e)}\")\n",
    "            self.embedding_model = None\n",
    "\n",
    "        self.emebeddings = None\n",
    "        self.passages = None\n",
    "\n",
    "    def get_article_passages(self, title, passage_length=3):\n",
    "        article_text = get_wikipedia_article(title)\n",
    "\n",
    "        if article_text:\n",
    "            cleaned_text = clean_text(article_text)\n",
    "            passages = split_into_passages(cleaned_text, sentences_per_passage=5)\n",
    "\n",
    "            # Display first 3 passages\n",
    "            for i, passage in enumerate(passages[:3]):\n",
    "                print(f\"Passage {i+1}:\\n{passage}\\n{'-'*50}\")\n",
    "\n",
    "        self.passages = passages\n",
    "\n",
    "    def embed(self, text):\n",
    "        \"\"\"\n",
    "        Embed a list of passages using the embedding model.\n",
    "        \"\"\"\n",
    "        if self.embedding_model is None:\n",
    "            raise ValueError(\"Embedding model not loaded\")\n",
    "        \n",
    "        # Tokenize the input\n",
    "        inputs = self.embedding_tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        inputs.to(self.device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.embedding_model(**inputs).last_hidden_state\n",
    "        \n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        outputs = outputs.masked_fill(~mask[..., None].bool(), 0.)\n",
    "        embeddings = outputs.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "        \n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    "    def create_embeddings(self):\n",
    "        \"\"\"\n",
    "        Create embeddings for a list of passages.\n",
    "        \"\"\"\n",
    "        if self.embedding_model is None:\n",
    "            raise ValueError(\"Embedding model not loaded\")\n",
    "\n",
    "        self.embeddings = self.embed(self.passages)\n",
    "        \n",
    "    def retrieve_passages(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Retrieve relevant passages based on semantic similarity.\n",
    "        \"\"\"\n",
    "        if self.embedding_model is None:\n",
    "            raise ValueError(\"Embedding model not loaded\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embed([query])\n",
    "        \n",
    "        # Generate passage embeddings if not provided\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"Passage embeddings not created\")\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top-k passages\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        top_passages = [self.passages[i] for i in top_indices]\n",
    "        top_scores = [similarities[i] for i in top_indices]\n",
    "        \n",
    "        return list(zip(top_passages, top_scores))\n",
    "    \n",
    "    def generate_response(self, query, context, max_length=512):\n",
    "        \"\"\"\n",
    "        Generate a response using the Llama model based on query and context.\n",
    "        \"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise ValueError(\"language model not loaded\")\n",
    "        \n",
    "        # Prepare prompt with context and query\n",
    "        prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "            You are a helpful assistant. Use the following context to answer the question at the end.\n",
    "            <</SYS>>\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {query} [/INST]\n",
    "            \"\"\"\n",
    "        \n",
    "        # Tokenize the prompt\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=inputs.input_ids.shape[1] + max_length,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Decode and return the response (excluding the prompt)\n",
    "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = full_response[len(self.tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)):].strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def rag_response(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        End-to-end RAG: Retrieve relevant passages and generate a response.\n",
    "        \"\"\"\n",
    "        # Get passages if not provided\n",
    "        if self.passages is None:\n",
    "            raise ValueError(\"passages are not ready\")\n",
    "        \n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"embeddings are not ready\")\n",
    "        \n",
    "        # Retrieve relevant passages\n",
    "        retrieved_passages = self.retrieve_passages(query, top_k=top_k)\n",
    "        \n",
    "        # Combine retrieved passages as context\n",
    "        context = \"\\n\\n\".join([p for p, _ in retrieved_passages])\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.generate_response(query, context)\n",
    "        \n",
    "        return response, retrieved_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: 1\n",
      "Loading Llama model from meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a3dba5072546a3bd44cd38f02c4bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama model loaded successfully\n",
      "Loading embedding model facebook/contriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cis/home/adesilva/miniconda3/envs/prol/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RAG system\n",
    "rag = RAG(device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage 1:\n",
      "The Australian Grand Prix is an annual Formula One motor racing event, taking place in Melbourne, Victoria. The event is contracted to be held at least until 2035. One of the oldest surviving motorsport competitions held in Australia, the Grand Prix has moved frequently with 23 different venues having been used since it was first run at Phillip Island in 1928. The race became part of the Formula One World Championship in 1985. Since 1996, it has been held at the Albert Park Circuit in Melbourne, with the exceptions of 2020 and 2021, when the races were cancelled due to the COVID-19 pandemic.\n",
      "--------------------------------------------------\n",
      "Passage 2:\n",
      "Before that, it was held in Adelaide. Historically, the Australian Grand Prix was held as either the last race of the season, when held at Adelaide, or as the opening round or early on at Melbourne. In 2022, it returned to the calendar as the third race of the season, following the Bahrain and Saudi Arabian Grands Prix. In 2025, the Grand Prix was moved back to the opening race slot of the season. History Pre-war While an event called the Australian Grand Prix was staged in 1927 at the grass surface Goulburn Racecourse held as a series of sprints, it is generally accepted that the Australian Grand Prix began as the 100 Miles Road Race held at the Phillip Island road circuit in 1928.\n",
      "--------------------------------------------------\n",
      "Passage 3:\n",
      "The inaugural race was won by Arthur Waite in what was effectively an entry supported by the Austin Motor Company, a modified Austin 7. For eight years, races, first called the Australian Grand Prix in 1929, continued on the rectangular dirt road circuit. Bugattis dominated the results, taking four consecutive wins from 1929 to 1932. The last Phillip Island race was in 1935 and the title lapsed for three years. An AGP style event was held on Boxing Day, 1936 at the South Australian town of Victor Harbor for a centennial South Australian Grand Prix before the Australian Grand Prix title was revived in 1938 for the grand opening of what would become one of the world's most famous race tracks, Mount Panorama just outside the semi-rural town of Bathurst.\n",
      "--------------------------------------------------\n",
      "Retrieved 49 passages from 'Australian Grand Prix'\n"
     ]
    }
   ],
   "source": [
    "# Get passages from an article\n",
    "article_title = \"Australian Grand Prix\"\n",
    "rag.get_article_passages(article_title)\n",
    "\n",
    "print(f\"Retrieved {len(rag.passages)} passages from '{article_title}'\")\n",
    "\n",
    "# Generate embeddings for passages (can be cached for repeated queries)\n",
    "rag.create_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved passages:\n",
      "\n",
      "Passage 1 (similarity: 0.5369):\n",
      "The new contract stipulates that the Australian Grand Prix will be one of the first three rounds of the season over the contract period and will host a minimum of five season-opening races over the 13 years between 2023 and 2035. From 2023, Formula 2 and Formula 3 races will form part of the race weekend schedule. A further two-year extension was signed in December 2022, ensuring that the race would remain in Melbourne until 2037. After a two-year absence as a result of the COVID-19 pandemic, the Australian Grand Prix returned in 2022. Unlike previous years, when it was the opening event of the season, the 2022 Australian Grand Prix was instead the third event of the season.\n",
      "\n",
      "Passage 2 (similarity: 0.5109):\n",
      "The Australian Grand Prix is an annual Formula One motor racing event, taking place in Melbourne, Victoria. The event is contracted to be held at least until 2035. One of the oldest surviving motorsport competitions held in Australia, the Grand Prix has moved frequently with 23 different venues having been used since it was first run at Phillip Island in 1928. The race became part of the Formula One World Championship in 1985. Since 1996, it has been held at the Albert Park Circuit in Melbourne, with the exceptions of 2020 and 2021, when the races were cancelled due to the COVID-19 pandemic.\n",
      "\n",
      "Passage 3 (similarity: 0.5096):\n",
      "Before that, it was held in Adelaide. Historically, the Australian Grand Prix was held as either the last race of the season, when held at Adelaide, or as the opening round or early on at Melbourne. In 2022, it returned to the calendar as the third race of the season, following the Bahrain and Saudi Arabian Grands Prix. In 2025, the Grand Prix was moved back to the opening race slot of the season. History Pre-war While an event called the Australian Grand Prix was staged in 1927 at the grass surface Goulburn Racecourse held as a series of sprints, it is generally accepted that the Australian Grand Prix began as the 100 Miles Road Race held at the Phillip Island road circuit in 1928.\n"
     ]
    }
   ],
   "source": [
    "# Process a query\n",
    "query = \"Who won the Australian Grand Prix in 2024?\"\n",
    "\n",
    "# Retrieve relevant passages\n",
    "retrieved_passages = rag.retrieve_passages(query, top_k=3)\n",
    "\n",
    "print(\"\\nRetrieved passages:\")\n",
    "for i, (passage, score) in enumerate(retrieved_passages):\n",
    "    print(f\"\\nPassage {i+1} (similarity: {score:.4f}):\")\n",
    "    print(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "context = \"\\n\\n\".join([p for p, _ in retrieved_passages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated response:\n",
      "According to the provided context, the Australian Grand Prix did not take place in 2024 due to the COVID-19 pandemic. Therefore, there was no winner for the Australian Grand Prix in 2024.\n"
     ]
    }
   ],
   "source": [
    "response = rag.generate_response(query, [context])\n",
    "\n",
    "print(\"\\nGenerated response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End-to-end RAG response:\n",
      "Based on the information provided in the context, the winner of the Australian Grand Prix in 2024 is not specified. The context only provides information about the race's history, the venue, and the contract extension until 2035, but does not mention the winner of any specific year, including 2024. Therefore, I cannot provide an answer to your question.\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, use the end-to-end method\n",
    "response, _ = rag.rag_response(query)\n",
    "\n",
    "print(\"\\nEnd-to-end RAG response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
