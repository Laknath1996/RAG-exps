{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Useful Links -----\n",
    "# https://github.com/AkariAsai/self-rag/tree/main?tab=readme-ov-file#updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /cis/home/adesilva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "import wikipediaapi\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_article(title, lang=\"en\"):\n",
    "    \"\"\"Fetch the Wikipedia article text.\"\"\"\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(user_agent='ashwin', language=lang)\n",
    "    page = wiki_wiki.page(title)\n",
    "    \n",
    "    if not page.exists():\n",
    "        print(f\"Page '{title}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    return page.text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove unwanted formatting, citations, and excessive whitespace.\"\"\"\n",
    "    text = re.sub(r\"\\[\\d+\\]\", \"\", text)  # Remove citation references like [1], [2]\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n",
    "    return text\n",
    "\n",
    "def split_into_passages(text, sentences_per_passage=5):\n",
    "    \"\"\"Split text into passages based on sentence count.\"\"\"\n",
    "    sentences = sent_tokenize(text)  # Tokenize into sentences\n",
    "    passages = [\n",
    "        \" \".join(sentences[i : i + sentences_per_passage])\n",
    "        for i in range(0, len(sentences), sentences_per_passage)\n",
    "    ]\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    def __init__(self, language_model=\"meta-llama/Llama-2-7b-chat-hf\", \n",
    "                 embedding_model=\"facebook/contriever\",\n",
    "                 device=None):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system\n",
    "        \"\"\"\n",
    "        # Set device (GPU if available, otherwise CPU)\n",
    "        if device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load Llama model and tokenizer\n",
    "        try:\n",
    "            print(f\"Loading Llama model from {language_model}...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(language_model)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                language_model,\n",
    "                torch_dtype=torch.float16,\n",
    "            ).to(self.device)\n",
    "            print(\"Llama model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Llama model: {str(e)}\")\n",
    "            print(\"You may need to login with 'huggingface-cli login' and request access to Llama models\")\n",
    "            self.tokenizer = None\n",
    "            self.model = None\n",
    "        \n",
    "        # Load embedding model\n",
    "        try:\n",
    "            print(f\"Loading embedding model {embedding_model}...\")\n",
    "            self.embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "            self.embedding_model = AutoModel.from_pretrained(embedding_model).to(self.device)\n",
    "            print(\"Embedding model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading embedding model: {str(e)}\")\n",
    "            self.embedding_model = None\n",
    "\n",
    "        self.emebeddings = None\n",
    "        self.passages = None\n",
    "\n",
    "    def get_article_passages(self, title, passage_length=3):\n",
    "        article_text = get_wikipedia_article(title)\n",
    "\n",
    "        if article_text:\n",
    "            cleaned_text = clean_text(article_text)\n",
    "            passages = split_into_passages(cleaned_text, sentences_per_passage=5)\n",
    "\n",
    "            # Display first 3 passages\n",
    "            for i, passage in enumerate(passages[:3]):\n",
    "                print(f\"Passage {i+1}:\\n{passage}\\n{'-'*50}\")\n",
    "\n",
    "        self.passages = passages\n",
    "\n",
    "    def embed(self, text):\n",
    "        \"\"\"\n",
    "        Embed a list of passages using the embedding model.\n",
    "        \"\"\"\n",
    "        if self.embedding_model is None:\n",
    "            raise ValueError(\"Embedding model not loaded\")\n",
    "        \n",
    "        # Tokenize the input\n",
    "        inputs = self.embedding_tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        inputs.to(self.device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.embedding_model(**inputs).last_hidden_state\n",
    "        \n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        outputs = outputs.masked_fill(~mask[..., None].bool(), 0.)\n",
    "        embeddings = outputs.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "        \n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    "    def create_embeddings(self):\n",
    "        \"\"\"\n",
    "        Create embeddings for a list of passages.\n",
    "        \"\"\"\n",
    "        if self.embedding_model is None:\n",
    "            raise ValueError(\"Embedding model not loaded\")\n",
    "\n",
    "        self.embeddings = self.embed(self.passages)\n",
    "        \n",
    "    def retrieve_passages(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Retrieve relevant passages based on semantic similarity.\n",
    "        \"\"\"\n",
    "        if self.embedding_model is None:\n",
    "            raise ValueError(\"Embedding model not loaded\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embed([query])\n",
    "        \n",
    "        # Generate passage embeddings if not provided\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"Passage embeddings not created\")\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top-k passages\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        top_passages = [self.passages[i] for i in top_indices]\n",
    "        top_scores = [similarities[i] for i in top_indices]\n",
    "        \n",
    "        return list(zip(top_passages, top_scores))\n",
    "    \n",
    "    def generate_response(self, query, context, max_length=512):\n",
    "        \"\"\"\n",
    "        Generate a response using the Llama model based on query and context.\n",
    "        \"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise ValueError(\"language model not loaded\")\n",
    "        \n",
    "        # Prepare prompt with context and query\n",
    "        prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "            You are a helpful assistant. Use the following context to answer the question at the end.\n",
    "            <</SYS>>\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {query} [/INST]\n",
    "            \"\"\"\n",
    "        \n",
    "        # Tokenize the prompt\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=inputs.input_ids.shape[1] + max_length,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Decode and return the response (excluding the prompt)\n",
    "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = full_response[len(self.tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)):].strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def rag_response(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        End-to-end RAG: Retrieve relevant passages and generate a response.\n",
    "        \"\"\"\n",
    "        # Get passages if not provided\n",
    "        if self.passages is None:\n",
    "            raise ValueError(\"passages are not ready\")\n",
    "        \n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"embeddings are not ready\")\n",
    "        \n",
    "        # Retrieve relevant passages\n",
    "        retrieved_passages = self.retrieve_passages(query, top_k=top_k)\n",
    "        \n",
    "        # Combine retrieved passages as context\n",
    "        context = \"\\n\\n\".join([p for p, _ in retrieved_passages])\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.generate_response(query, context)\n",
    "        \n",
    "        return response, retrieved_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: 1\n",
      "Loading Llama model from meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6345507c199a442fb66ace20eadebbe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama model loaded successfully\n",
      "Loading embedding model facebook/contriever...\n",
      "Embedding model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RAG system\n",
    "rag = RAG(device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage 1:\n",
      "The Australian Grand Prix is an annual Formula One motor racing event, taking place in Melbourne, Victoria. The event is contracted to be held at least until 2035. One of the oldest surviving motorsport competitions held in Australia, the Grand Prix has moved frequently with 23 different venues having been used since it was first run at Phillip Island in 1928. The race became part of the Formula One World Championship in 1985. Since 1996, it has been held at the Albert Park Circuit in Melbourne, with the exceptions of 2020 and 2021, when the races were cancelled due to the COVID-19 pandemic.\n",
      "--------------------------------------------------\n",
      "Passage 2:\n",
      "Before that, it was held in Adelaide. History Pre-war While an event called the Australian Grand Prix was staged in 1927 at the grass surface Goulburn Racecourse held as a series of sprints, it is generally accepted that the Australian Grand Prix began as the 100 Miles Road Race held at the Phillip Island road circuit in 1928. The inaugural race was won by Arthur Waite in what was effectively an entry supported by the Austin Motor Company, a modified Austin 7. For eight years, races, first called the Australian Grand Prix in 1929, continued on the rectangular dirt road circuit. Bugattis dominated the results, taking four consecutive wins from 1929 to 1932.\n",
      "--------------------------------------------------\n",
      "Passage 3:\n",
      "The last Phillip Island race was in 1935 and the title lapsed for three years. An AGP style event was held on Boxing Day, 1936 at the South Australian town of Victor Harbor for a centennial South Australian Grand Prix before the Australian Grand Prix title was revived in 1938 for the grand opening of what would become one of the world's most famous race tracks, Mount Panorama just outside the semi-rural town of Bathurst. Only just completed, with a tar seal for the circuit still a year away, the race was won by Englishman Peter Whitehead racing a new voiturette ERA B-Type that was just too fast for the locally developed machinery. One more race was held, at the Lobethal Circuit near the South Australian town of Lobethal in 1939, before the country was plunged into World War II. Post-war Early post-war races In the immediate post-war era, racing was sparse with competitors using pre-war cars with supplies cobbled together around the rationing of fuel and tyres.\n",
      "--------------------------------------------------\n",
      "Retrieved 48 passages from 'Australian Grand Prix'\n"
     ]
    }
   ],
   "source": [
    "# Get passages from an article\n",
    "article_title = \"Australian Grand Prix\"\n",
    "rag.get_article_passages(article_title)\n",
    "\n",
    "print(f\"Retrieved {len(rag.passages)} passages from '{article_title}'\")\n",
    "\n",
    "# Generate embeddings for passages (can be cached for repeated queries)\n",
    "rag.create_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved passages:\n",
      "\n",
      "Passage 1 (similarity: 0.2346):\n",
      "Several other corners were reprofiled to encourage overtaking, most notably the old turn 13, which was widened to create additional racing lines. Positive camber was also added to allow drivers to carry more speed through the corner. The main straight and pit lane were also redesigned, with the pit lane wall moved two metres closer to the circuit so that the edge of the circuit sat directly next to the wall. The 2022 Grand Prix saw Ferrari's Charles Leclerc achieve his first career grand slam, having started in pole position, set the fastest lap, led every lap, and won the race ahead of Red Bull's Sergio Pérez and Mercedes' George Russell. It was the first grand slam for an individual Ferrari driver since Fernando Alonso's at the 2010 Singapore Grand Prix.\n",
      "\n",
      "Passage 2 (similarity: 0.2276):\n",
      "The 2022 edition set a new attendance record at the circuit for the weekend, with a reported 419,114 attendees, including 128,294 on race day; these figures made the 2022 Grand Prix the highest attended Grand Prix ever held in Melbourne and one of the most popular sporting weekends in Australian history. The 2023 edition, which saw Max Verstappen win his maiden Australian Grand Prix, would break the record with 444,631 attendees, and would also break a Formula One record; the race, which ended with twelve cars left running, is the first ever to have three red flags throughout the session. This came after a chaotic race that saw many incidents; the Formula 2 and Formula 3 races, held the same weekend, had a similar outcome. The 2024 Australian Grand Prix, won by Carlos Sainz Jr., was notable for breaking numerous attendance records. The event sold out for the first time in its history, and set a new attendance record at the circuit for the weekend with 452,055 spectators, making it the most attended sporting event ever in Melbourne, and the fourth highest attended Formula One Grand Prix of all time, following the 1995 edition of the race, held in Adelaide, with 520,000 attendees, and the 2023 and 2024 British Grands Prix, which both drew 480,000 attendees.\n",
      "\n",
      "Passage 3 (similarity: 0.2093):\n",
      "A pink background indicates an event which was not part of the Formula One World Championship. Repeat winners (engine manufacturers) Manufacturers in bold are competing in the Formula One championship in the current season. A pink background indicates an event which was not part of the Formula One World Championship. * Between 1997 and 2003 built by Ilmor ** Between 1968 and 1993 designed and built by Cosworth, funded by Ford By year A pink background indicates an event which was not part of the Formula One World Championship. * From 1932 to 1948, the winner was determined on a handicap basis.\n"
     ]
    }
   ],
   "source": [
    "# Process a query\n",
    "query = \"Who is Carlos Sainz Jr.?\"\n",
    "\n",
    "# Retrieve relevant passages\n",
    "retrieved_passages = rag.retrieve_passages(query, top_k=3)\n",
    "\n",
    "print(\"\\nRetrieved passages:\")\n",
    "for i, (passage, score) in enumerate(retrieved_passages):\n",
    "    print(f\"\\nPassage {i+1} (similarity: {score:.4f}):\")\n",
    "    print(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "context = \"\\n\\n\".join([p for p, _ in retrieved_passages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated response:\n",
      "Carlos Sainz Jr. is a Formula One driver who won the 2024 Australian Grand Prix. He is a Spanish driver who has been competing in the Formula One World Championship since 2015, driving for teams such as Toro Rosso and Ferrari. Sainz Jr. is the son of two-time Formula One World Champion Carlos Sainz Sr. and has had a successful career in the sport, achieving several podium finishes and setting fast lap times throughout his career.\n"
     ]
    }
   ],
   "source": [
    "response = rag.generate_response(query, [context])\n",
    "\n",
    "print(\"\\nGenerated response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End-to-end RAG response:\n",
      "Carlos Sainz Jr. is a Spanish racing driver who won the 2024 Australian Grand Prix.\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, use the end-to-end method\n",
    "response, _ = rag.rag_response(query)\n",
    "\n",
    "print(\"\\nEnd-to-end RAG response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
