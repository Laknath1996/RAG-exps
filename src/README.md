### Todo List


* Write the training script
* ✅ Generate prospective questions using Gemini
* Use HF tokenizer to tokenizer the gutenberg dataset
* configure the GPT2 model according to https://huggingface.co/learn/llm-course/en/chapter7/6
* ✅ Run the model over a long book and compute the perplexity (sanity check)
* ✅ Create a tokenized dataset of english fiction books 
* ✅ List down the token margins of each book
* ✅ write a training script
* ✅ write perplexity script
* ✅ Do a full run
* ✅ Make a plot